{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bala-24g/Brain_tumour_classification/blob/main/Brain_tumour_accordingtopaper_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRqJYyjGA4Hp",
        "outputId": "c0709475-fca5-44db-c319-a8fe22294ef3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-fuzzy\n",
            "  Downloading scikit_fuzzy-0.5.0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Downloading scikit_fuzzy-0.5.0-py2.py3-none-any.whl (920 kB)\n",
            "   ---------------------------------------- 0.0/920.8 kB ? eta -:--:--\n",
            "   --------------------------------------- 920.8/920.8 kB 14.3 MB/s eta 0:00:00\n",
            "Installing collected packages: scikit-fuzzy\n",
            "Successfully installed scikit-fuzzy-0.5.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        " pip install -U scikit-fuzzy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cs2cCHKuBRxZ",
        "outputId": "1198d7d2-4980-4931-ae98-c8f543d5273d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting PyWaveletsNote: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "  Downloading pywavelets-1.7.0-cp312-cp312-win_amd64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\nezes\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from PyWavelets) (1.26.3)\n",
            "Downloading pywavelets-1.7.0-cp312-cp312-win_amd64.whl (4.2 MB)\n",
            "   ---------------------------------------- 0.0/4.2 MB ? eta -:--:--\n",
            "   --------- ------------------------------ 1.0/4.2 MB 12.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 4.2/4.2 MB 12.1 MB/s eta 0:00:00\n",
            "Installing collected packages: PyWavelets\n",
            "Successfully installed PyWavelets-1.7.0\n"
          ]
        }
      ],
      "source": [
        "pip install PyWavelets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rAB7vieBBxu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import skfuzzy as fuzz\n",
        "import pywt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlO_JaLxz70S",
        "outputId": "01d7477d-0afc-4ce3-f070-9a40337a3fdd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 826/826 [00:01<00:00, 537.35it/s]\n",
            "100%|██████████| 822/822 [00:01<00:00, 573.41it/s]\n",
            "100%|██████████| 395/395 [00:00<00:00, 593.69it/s]\n",
            "100%|██████████| 827/827 [00:01<00:00, 447.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data size: 2870\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 638.27it/s]\n",
            "100%|██████████| 115/115 [00:00<00:00, 815.65it/s]\n",
            "100%|██████████| 105/105 [00:00<00:00, 1447.27it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 246.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing data size: 394\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\nezes\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3565 - loss: 2.9900 - val_accuracy: 0.4477 - val_loss: 1.2225\n",
            "Epoch 2/20\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6056 - loss: 0.9704 - val_accuracy: 0.5174 - val_loss: 1.1657\n",
            "Epoch 3/20\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7291 - loss: 0.6939 - val_accuracy: 0.5070 - val_loss: 1.2242\n",
            "Epoch 4/20\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7863 - loss: 0.5686 - val_accuracy: 0.5801 - val_loss: 1.1514\n",
            "Epoch 5/20\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8487 - loss: 0.4106 - val_accuracy: 0.5575 - val_loss: 1.2214\n",
            "Epoch 6/20\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8951 - loss: 0.3239 - val_accuracy: 0.5819 - val_loss: 1.3738\n",
            "Epoch 7/20\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9132 - loss: 0.2578 - val_accuracy: 0.5418 - val_loss: 1.5855\n",
            "Epoch 8/20\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8995 - loss: 0.2735 - val_accuracy: 0.6202 - val_loss: 1.4822\n",
            "Epoch 9/20\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9631 - loss: 0.1390 - val_accuracy: 0.6185 - val_loss: 1.4985\n",
            "Epoch 10/20\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9630 - loss: 0.1150 - val_accuracy: 0.5714 - val_loss: 1.8127\n",
            "Epoch 11/20\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9232 - loss: 0.1931 - val_accuracy: 0.5993 - val_loss: 1.6514\n",
            "Epoch 12/20\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9420 - loss: 0.1634 - val_accuracy: 0.6132 - val_loss: 1.8428\n",
            "Epoch 13/20\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9594 - loss: 0.1154 - val_accuracy: 0.5923 - val_loss: 1.7785\n",
            "Epoch 14/20\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9656 - loss: 0.1138 - val_accuracy: 0.6115 - val_loss: 1.9708\n",
            "Epoch 15/20\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9785 - loss: 0.0691 - val_accuracy: 0.6115 - val_loss: 1.7428\n",
            "Epoch 16/20\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9821 - loss: 0.0729 - val_accuracy: 0.6220 - val_loss: 1.9096\n",
            "Epoch 17/20\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9867 - loss: 0.0506 - val_accuracy: 0.6045 - val_loss: 2.0998\n",
            "Epoch 18/20\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9700 - loss: 0.0867 - val_accuracy: 0.6080 - val_loss: 2.1519\n",
            "Epoch 19/20\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9637 - loss: 0.1064 - val_accuracy: 0.5836 - val_loss: 2.1444\n",
            "Epoch 20/20\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9568 - loss: 0.1162 - val_accuracy: 0.5889 - val_loss: 2.1580\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4190 - loss: 3.9826 \n",
            "Test Accuracy: 41.12%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from sklearn.decomposition import PCA\n",
        "import pywt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import skfuzzy as fuzz  # Now this should work\n",
        "\n",
        "# Define necessary constants\n",
        "TEST_DIR = 'C:\\\\Users\\\\nezes\\\\Documents\\\\Github_projects\\\\Brain_tumour\\\\Brain-Tumor-Classification-DataSet\\\\Testing'\n",
        "TRAIN_DIR = 'C:\\\\Users\\\\nezes\\\\Documents\\\\Github_projects\\\\Brain_tumour\\\\Brain-Tumor-Classification-DataSet\\\\Training'\n",
        "IMG_SIZE = 224\n",
        "\n",
        "CATEGORIES = [\"glioma_tumor\", \"meningioma_tumor\", \"no_tumor\", \"pituitary_tumor\"]\n",
        "\n",
        "# Creating training dataset\n",
        "training_data = []\n",
        "\n",
        "def create_training_data():\n",
        "    for category in CATEGORIES:\n",
        "        path = os.path.join(TRAIN_DIR, category)\n",
        "        class_num = CATEGORIES.index(category)\n",
        "        for img in tqdm(os.listdir(path)):\n",
        "            img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_COLOR)\n",
        "            new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
        "            training_data.append([new_array, class_num])\n",
        "    random.shuffle(training_data)\n",
        "\n",
        "create_training_data()\n",
        "print(\"Training data size:\", len(training_data))\n",
        "\n",
        "# Preparing training data\n",
        "X_train = np.array([i[0] for i in training_data]).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n",
        "Y_train = np.array([i[1] for i in training_data])\n",
        "\n",
        "# Creating testing dataset\n",
        "testing_data = []\n",
        "\n",
        "def create_testing_data():\n",
        "    for category in CATEGORIES:\n",
        "        path = os.path.join(TEST_DIR, category)\n",
        "        class_num = CATEGORIES.index(category)\n",
        "\n",
        "        for img in tqdm(os.listdir(path)):\n",
        "            img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_COLOR)\n",
        "            new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
        "            testing_data.append([new_array, class_num])\n",
        "    random.shuffle(testing_data)\n",
        "\n",
        "create_testing_data()\n",
        "print(\"Testing data size:\", len(testing_data))\n",
        "\n",
        "# Preparing testing data\n",
        "X_test = np.array([i[0] for i in testing_data]).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n",
        "Y_test = np.array([i[1] for i in testing_data])\n",
        "\n",
        "# Step 3: Image Segmentation using Fuzzy C-means\n",
        "def fuzzy_c_means_clustering(img, num_clusters=5):\n",
        "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    img_flat = img_gray.flatten()\n",
        "    img_norm = img_flat / 255.0\n",
        "    cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(img_norm.reshape(1, -1), num_clusters, 2, error=0.005, maxiter=1000)\n",
        "    cluster_membership = np.argmax(u, axis=0)\n",
        "    return cluster_membership.reshape(img_gray.shape)\n",
        "\n",
        "# Apply segmentation to training and test data\n",
        "X_train_segmented = np.array([fuzzy_c_means_clustering(img) for img in X_train])\n",
        "X_test_segmented = np.array([fuzzy_c_means_clustering(img) for img in X_test])\n",
        "\n",
        "# Step 4: Feature Extraction using Discrete Wavelet Transform (DWT)\n",
        "def dwt_feature_extraction(img, wavelet='haar', level=3):\n",
        "    coeffs = pywt.wavedec2(img, wavelet, level=level)\n",
        "    coeff_arr, coeff_slices = pywt.coeffs_to_array(coeffs)\n",
        "    return coeff_arr\n",
        "\n",
        "# Extract features using DWT\n",
        "X_train_features = np.array([dwt_feature_extraction(img) for img in X_train_segmented])\n",
        "X_test_features = np.array([dwt_feature_extraction(img) for img in X_test_segmented])\n",
        "\n",
        "# Step 5: Feature Reduction using PCA\n",
        "pca = PCA(n_components=100)  # Adjust components based on your dataset\n",
        "X_train_reduced = pca.fit_transform(X_train_features.reshape(X_train_features.shape[0], -1))\n",
        "X_test_reduced = pca.transform(X_test_features.reshape(X_test_features.shape[0], -1))\n",
        "\n",
        "# Step 6: Classification using DNN\n",
        "def build_dnn_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(512, input_shape=(input_shape,), activation='relu'))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(len(CATEGORIES), activation='softmax'))  # Number of classes\n",
        "\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build and train the DNN\n",
        "input_shape = X_train_reduced.shape[1]\n",
        "model = build_dnn_model(input_shape)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_reduced, Y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Step 7: Evaluate on Test Set\n",
        "loss, accuracy = model.evaluate(X_test_reduced, Y_test)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEgHSU2Zzbiu"
      },
      "outputs": [],
      "source": [
        "# Clone the dataset from GitHub\n",
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from sklearn.decomposition import PCA\n",
        "import pywt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import skfuzzy as fuzz\n",
        "\n",
        "# Define necessary constants\n",
        "TEST_DIR = '/content/Brain-Tumor-Classification-DataSet/Testing'\n",
        "TRAIN_DIR = '/content/Brain-Tumor-Classification-DataSet/Training'\n",
        "IMG_SIZE = 224\n",
        "CATEGORIES = [\"glioma_tumor\", \"meningioma_tumor\", \"no_tumor\", \"pituitary_tumor\"]\n",
        "\n",
        "# Creating training dataset\n",
        "training_data = []\n",
        "\n",
        "def create_training_data():\n",
        "    for category in CATEGORIES:\n",
        "        path = os.path.join(TRAIN_DIR, category)\n",
        "        class_num = CATEGORIES.index(category)\n",
        "        for img in tqdm(os.listdir(path)):\n",
        "            img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_COLOR)\n",
        "            new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
        "            training_data.append([new_array, class_num])\n",
        "    random.shuffle(training_data)\n",
        "\n",
        "create_training_data()\n",
        "print(\"Training data size:\", len(training_data))\n",
        "\n",
        "# Preparing training data\n",
        "X_train = np.array([i[0] for i in training_data]).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n",
        "Y_train = np.array([i[1] for i in training_data])\n",
        "\n",
        "# Creating testing dataset\n",
        "testing_data = []\n",
        "\n",
        "def create_testing_data():\n",
        "    for category in CATEGORIES:\n",
        "        path = os.path.join(TEST_DIR, category)\n",
        "        class_num = CATEGORIES.index(category)\n",
        "\n",
        "        for img in tqdm(os.listdir(path)):\n",
        "            img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_COLOR)\n",
        "            new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
        "            testing_data.append([new_array, class_num])\n",
        "    random.shuffle(testing_data)\n",
        "\n",
        "create_testing_data()\n",
        "print(\"Testing data size:\", len(testing_data))\n",
        "\n",
        "# Preparing testing data\n",
        "X_test = np.array([i[0] for i in testing_data]).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n",
        "Y_test = np.array([i[1] for i in testing_data])\n",
        "\n",
        "# Step 3: Image Segmentation using Fuzzy C-means\n",
        "def fuzzy_c_means_clustering(img, num_clusters=5):\n",
        "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    img_flat = img_gray.flatten()\n",
        "    img_norm = img_flat / 255.0\n",
        "    cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(img_norm.reshape(1, -1), num_clusters, 2, error=0.005, maxiter=1000)\n",
        "    cluster_membership = np.argmax(u, axis=0)\n",
        "    return cluster_membership.reshape(img_gray.shape)\n",
        "\n",
        "# Apply segmentation to training and test data\n",
        "X_train_segmented = np.array([fuzzy_c_means_clustering(img) for img in X_train])\n",
        "X_test_segmented = np.array([fuzzy_c_means_clustering(img) for img in X_test])\n",
        "\n",
        "# Step 4: Feature Extraction using Discrete Wavelet Transform (DWT)\n",
        "def dwt_feature_extraction(img, wavelet='haar', level=3):\n",
        "    coeffs = pywt.wavedec2(img, wavelet, level=level)\n",
        "    coeff_arr, coeff_slices = pywt.coeffs_to_array(coeffs)\n",
        "    return coeff_arr\n",
        "\n",
        "# Extract features using DWT\n",
        "X_train_features = np.array([dwt_feature_extraction(img) for img in X_train_segmented])\n",
        "X_test_features = np.array([dwt_feature_extraction(img) for img in X_test_segmented])\n",
        "\n",
        "# Step 5: Feature Reduction using PCA\n",
        "pca = PCA(n_components=100)  # Adjust components based on your dataset\n",
        "X_train_reduced = pca.fit_transform(X_train_features.reshape(X_train_features.shape[0], -1))\n",
        "X_test_reduced = pca.transform(X_test_features.reshape(X_test_features.shape[0], -1))\n",
        "\n",
        "# Step 6: Classification using DNN\n",
        "def build_dnn_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(512, input_shape=(input_shape,), activation='relu'))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(len(CATEGORIES), activation='softmax'))  # Number of classes\n",
        "\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build and train the DNN\n",
        "input_shape = X_train_reduced.shape[1]\n",
        "model = build_dnn_model(input_shape)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_reduced, Y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Step 7: Evaluate on Test Set\n",
        "loss, accuracy = model.evaluate(X_test_reduced, Y_test)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1RR13qxwiyV"
      },
      "outputs": [],
      "source": [
        "def fuzzy_c_means_segmentation(image, n_clusters=5, m=2, error=0.005, maxiter=1000):\n",
        "    \"\"\"\n",
        "    Perform Fuzzy C-means clustering on a grayscale image.\n",
        "\n",
        "    Parameters:\n",
        "        image (numpy.ndarray): Grayscale image.\n",
        "        n_clusters (int): Number of clusters.\n",
        "        m (float): Fuzziness parameter.\n",
        "        error (float): Stopping criterion.\n",
        "        maxiter (int): Maximum number of iterations.\n",
        "\n",
        "    Returns:\n",
        "        segmented_image (numpy.ndarray): Segmented image with cluster labels.\n",
        "    \"\"\"\n",
        "    # Flatten the image\n",
        "    data = image.flatten().astype(np.float64)\n",
        "\n",
        "    # Normalize data\n",
        "    data_normalized = (data - data.min()) / (data.max() - data.min())\n",
        "\n",
        "    # Transpose data for skfuzzy\n",
        "    data_transposed = np.expand_dims(data_normalized, axis=0)\n",
        "\n",
        "    # Fuzzy C-means\n",
        "    cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(\n",
        "        data_transposed, c=n_clusters, m=m, error=error, maxiter=maxiter, init=None)\n",
        "\n",
        "    # Get the cluster membership for each data point\n",
        "    cluster_membership = np.argmax(u, axis=0)\n",
        "\n",
        "    # Reshape to original image shape\n",
        "    segmented_image = cluster_membership.reshape(image.shape)\n",
        "\n",
        "    return segmented_image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VT3w8sHiwnO7"
      },
      "outputs": [],
      "source": [
        "def extract_dwt_features(image, wavelet='haar', level=3):\n",
        "    \"\"\"\n",
        "    Extract DWT features from an image.\n",
        "\n",
        "    Parameters:\n",
        "        image (numpy.ndarray): Grayscale image.\n",
        "        wavelet (str): Wavelet type.\n",
        "        level (int): Decomposition level.\n",
        "\n",
        "    Returns:\n",
        "        features (numpy.ndarray): Flattened DWT coefficients.\n",
        "    \"\"\"\n",
        "    coeffs = pywt.wavedec2(image, wavelet=wavelet, level=level)\n",
        "    # Extract coefficients from all levels\n",
        "    features = []\n",
        "    for coeff in coeffs:\n",
        "        if isinstance(coeff, tuple):\n",
        "            for sub_coeff in coeff:\n",
        "                features.extend(sub_coeff.flatten())\n",
        "        else:\n",
        "            features.extend(coeff.flatten())\n",
        "    return np.array(features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKpOOIdOwrM1"
      },
      "outputs": [],
      "source": [
        "def load_dataset(dataset_path, image_size=(256, 256)):\n",
        "    \"\"\"\n",
        "    Load images and labels from the dataset directory.\n",
        "\n",
        "    Parameters:\n",
        "        dataset_path (str): Path to the dataset directory.\n",
        "        image_size (tuple): Desired image size (width, height).\n",
        "\n",
        "    Returns:\n",
        "        images (list): List of preprocessed images.\n",
        "        labels (list): Corresponding labels.\n",
        "    \"\"\"\n",
        "    classes = ['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for label, class_name in enumerate(classes):\n",
        "        class_dir = os.path.join(dataset_path, class_name)\n",
        "        for filename in os.listdir(class_dir):\n",
        "            if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):\n",
        "                img_path = os.path.join(class_dir, filename)\n",
        "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "                if img is not None:\n",
        "                    img_resized = cv2.resize(img, image_size)\n",
        "                    images.append(img_resized)\n",
        "                    labels.append(label)\n",
        "    return images, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjX5boSUwuTy",
        "outputId": "45bf5dd3-4f69-415b-ac9a-884f13dc504e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total images: 2870\n",
            "Total labels: 2870\n"
          ]
        }
      ],
      "source": [
        "# Define dataset path\n",
        "dataset_path =  'C:\\\\Users\\\\nezes\\\\Documents\\\\Github_projects\\\\Brain_tumour\\\\Brain-Tumor-Classification-DataSet\\\\Training'  # Replace with your dataset path\n",
        "\n",
        "# Load images and labels\n",
        "images, labels = load_dataset(dataset_path)\n",
        "\n",
        "print(f\"Total images: {len(images)}\")\n",
        "print(f\"Total labels: {len(labels)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1tpJLqkCN0b",
        "outputId": "f50a53e0-7e0a-44bd-ccbc-63fa4850eb55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "print(labels[0])\n",
        "print(labels[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8avyUJZ6w3BC",
        "outputId": "aa015cc8-fd45-41eb-d491-00d7467d3311"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted features shape: (2870, 65536)\n",
            "Labels shape: (2870,)\n"
          ]
        }
      ],
      "source": [
        "# Initialize list to hold features\n",
        "feature_list = []\n",
        "label_list = []\n",
        "\n",
        "for idx, img in enumerate(images):\n",
        "    # Image Segmentation\n",
        "    segmented = fuzzy_c_means_segmentation(img, n_clusters=5)\n",
        "\n",
        "    # Extract the tumor part\n",
        "    # Assuming the tumor corresponds to one of the clusters; you might need to adjust this\n",
        "    # For simplicity, let's take the cluster with the highest intensity as tumor\n",
        "    cluster_means = []\n",
        "    for c in range(5):\n",
        "        cluster_mean = img[segmented == c].mean() if np.any(segmented == c) else 0\n",
        "        cluster_means.append(cluster_mean)\n",
        "    tumor_cluster = np.argmax(cluster_means)\n",
        "    tumor_segment = np.zeros_like(img)\n",
        "    tumor_segment[segmented == tumor_cluster] = img[segmented == tumor_cluster]\n",
        "\n",
        "    # Feature Extraction using DWT\n",
        "    dwt_features = extract_dwt_features(tumor_segment, wavelet='haar', level=3)\n",
        "\n",
        "    feature_list.append(dwt_features)\n",
        "    label_list.append(labels[idx])\n",
        "\n",
        "# Convert to numpy arrays\n",
        "features = np.array(feature_list)\n",
        "labels = np.array(label_list)\n",
        "\n",
        "print(f\"Extracted features shape: {features.shape}\")\n",
        "print(f\"Labels shape: {labels.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mtju7KSw39-",
        "outputId": "a6755842-8679-4d6c-db6e-d5d5e0206977"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PCA features shape: (2870, 1024)\n"
          ]
        }
      ],
      "source": [
        "# Initialize PCA to reduce to 1024 features (if original features > 1024)\n",
        "pca = PCA(n_components=1024)\n",
        "features_pca = pca.fit_transform(features)\n",
        "\n",
        "print(f\"PCA features shape: {features_pca.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KDb7eZEw_tQ"
      },
      "outputs": [],
      "source": [
        "# One-hot encode labels for DNN\n",
        "num_classes = 4\n",
        "labels_categorical = to_categorical(labels, num_classes)\n",
        "\n",
        "# Define stratified k-fold\n",
        "skf = StratifiedKFold(n_splits=7, shuffle=True, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2wdY684xEQH"
      },
      "outputs": [],
      "source": [
        "# One-hot encode labels for DNN\n",
        "num_classes = 4\n",
        "labels_categorical = to_categorical(labels, num_classes)\n",
        "\n",
        "# Define stratified k-fold\n",
        "skf = StratifiedKFold(n_splits=7, shuffle=True, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkSHWFX3xH5w",
        "outputId": "fcd3292c-1ba9-43e9-d040-0b42cb896232"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\nezes\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DNN Accuracy: 0.2875\n",
            "KNN-1 Accuracy: 0.3659\n",
            "KNN-3 Accuracy: 0.3432\n",
            "LDA Accuracy: 0.4111\n",
            "------------------------------\n",
            "Fold 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\nezes\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DNN Accuracy: 0.2875\n",
            "KNN-1 Accuracy: 0.4861\n",
            "KNN-3 Accuracy: 0.4547\n",
            "LDA Accuracy: 0.4983\n",
            "------------------------------\n",
            "Fold 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\nezes\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DNN Accuracy: 0.2875\n",
            "KNN-1 Accuracy: 0.5314\n",
            "KNN-3 Accuracy: 0.5261\n",
            "LDA Accuracy: 0.5192\n",
            "------------------------------\n",
            "Fold 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\nezes\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DNN Accuracy: 0.2875\n",
            "KNN-1 Accuracy: 0.4930\n",
            "KNN-3 Accuracy: 0.4721\n",
            "LDA Accuracy: 0.4721\n",
            "------------------------------\n",
            "Fold 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\nezes\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DNN Accuracy: 0.2875\n",
            "KNN-1 Accuracy: 0.4390\n",
            "KNN-3 Accuracy: 0.3955\n",
            "LDA Accuracy: 0.4930\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Initialize lists to store accuracies\n",
        "dnn_accuracies = []\n",
        "knn1_accuracies = []\n",
        "knn3_accuracies = []\n",
        "lda_accuracies = []\n",
        "\n",
        "# Assuming you have defined `features_pca`, `labels`, and `labels_categorical`\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "\n",
        "for fold, (train_index, test_index) in enumerate(skf.split(features_pca, labels)):\n",
        "    print(f\"Fold {fold + 1}\")\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test = features_pca[train_index], features_pca[test_index]\n",
        "    y_train, y_test = labels[train_index], labels[test_index]\n",
        "    y_train_cat, y_test_cat = labels_categorical[train_index], labels_categorical[test_index]\n",
        "\n",
        "    # -------------------\n",
        "    # Deep Neural Network\n",
        "    # -------------------\n",
        "    model = Sequential()\n",
        "    model.add(Dense(512, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Train model\n",
        "    history = model.fit(X_train, y_train_cat,\n",
        "                        epochs=50,\n",
        "                        batch_size=32,\n",
        "                        verbose=0,\n",
        "                        validation_data=(X_test, y_test_cat))\n",
        "\n",
        "    # Evaluate model\n",
        "    loss, acc = model.evaluate(X_test, y_test_cat, verbose=0)\n",
        "    print(f\"DNN Accuracy: {acc:.4f}\")\n",
        "    dnn_accuracies.append(acc)\n",
        "\n",
        "    # -------------------\n",
        "    # K-Nearest Neighbors (K=1 & K=3)\n",
        "    # -------------------\n",
        "    knn1 = KNeighborsClassifier(n_neighbors=1)\n",
        "    knn1.fit(X_train, y_train)\n",
        "    knn1_pred = knn1.predict(X_test)\n",
        "    acc_knn1 = accuracy_score(y_test, knn1_pred)\n",
        "    print(f\"KNN-1 Accuracy: {acc_knn1:.4f}\")\n",
        "    knn1_accuracies.append(acc_knn1)\n",
        "\n",
        "    knn3 = KNeighborsClassifier(n_neighbors=3)\n",
        "    knn3.fit(X_train, y_train)\n",
        "    knn3_pred = knn3.predict(X_test)\n",
        "    acc_knn3 = accuracy_score(y_test, knn3_pred)\n",
        "    print(f\"KNN-3 Accuracy: {acc_knn3:.4f}\")\n",
        "    knn3_accuracies.append(acc_knn3)\n",
        "\n",
        "    # -------------------\n",
        "    # Linear Discriminant Analysis\n",
        "    # -------------------\n",
        "    lda = LinearDiscriminantAnalysis()\n",
        "    lda.fit(X_train, y_train)\n",
        "    lda_pred = lda.predict(X_test)\n",
        "    acc_lda = accuracy_score(y_test, lda_pred)\n",
        "    print(f\"LDA Accuracy: {acc_lda:.4f}\")\n",
        "    lda_accuracies.append(acc_lda)\n",
        "\n",
        "    print(\"-\" * 30)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jt3u5c5oxLBe",
        "outputId": "04c79f93-c8fb-4c1d-fcf6-c895dc27bdfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Accuracies over 7 folds:\n",
            "DNN: 0.2875 ± 0.0000\n",
            "KNN-1: 0.4631 ± 0.0568\n",
            "KNN-3: 0.4383 ± 0.0633\n",
            "LDA: 0.4787 ± 0.0370\n"
          ]
        }
      ],
      "source": [
        "print(\"Average Accuracies over 7 folds:\")\n",
        "print(f\"DNN: {np.mean(dnn_accuracies):.4f} ± {np.std(dnn_accuracies):.4f}\")\n",
        "print(f\"KNN-1: {np.mean(knn1_accuracies):.4f} ± {np.std(knn1_accuracies):.4f}\")\n",
        "print(f\"KNN-3: {np.mean(knn3_accuracies):.4f} ± {np.std(knn3_accuracies):.4f}\")\n",
        "print(f\"LDA: {np.mean(lda_accuracies):.4f} ± {np.std(lda_accuracies):.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}